{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1983.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>54.988401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.325831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>46.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>56.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>65.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>93.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Age\n",
       "count  1983.000000\n",
       "mean     54.988401\n",
       "std      14.325831\n",
       "min      18.000000\n",
       "25%      46.000000\n",
       "50%      56.000000\n",
       "75%      65.000000\n",
       "max      93.000000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "demographic_df = pd.read_csv('https://www.physionet.org/physiobank/database/challenge/2018/age-sex.csv')\n",
    "demographic_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1983"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(demographic_df)  # 1983"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'https://www.physionet.org/physiobank/database/challenge/2018/training/tr03-0005/.hea'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f61dedce5d37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Demo 1 - Read a wfdb record using the 'rdrecord' function into a wfdb.Record object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Plot the signals, and show the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mrecord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwfdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://www.physionet.org/physiobank/database/challenge/2018/training/tr03-0005/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mwfdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_wfdb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Record a103l from Physionet Challenge 2015'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/wfdb/io/record.py\u001b[0m in \u001b[0;36mrdrecord\u001b[0;34m(record_name, sampfrom, sampto, channels, physical, pb_dir, m2s, smooth_frames, ignore_skew, return_res)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m     \u001b[0;31m# Read the header fields\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 945\u001b[0;31m     \u001b[0mrecord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdheader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpb_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpb_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrd_segments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m     \u001b[0;31m# Set defaults for sampto and channels input variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/wfdb/io/record.py\u001b[0m in \u001b[0;36mrdheader\u001b[0;34m(record_name, pb_dir, rd_segments)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m     \u001b[0;31m# Read the header file. Separate comment and non-comment lines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m     \u001b[0mheader_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomment_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_header\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_header_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpb_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m     \u001b[0;31m# Get fields from record line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/wfdb/io/_header.py\u001b[0m in \u001b[0;36mget_header_lines\u001b[0;34m(record_name, pb_dir)\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;31m# Read local file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpb_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".hea\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m             \u001b[0;31m# Record line followed by signal/segment lines if any\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0mheader_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'https://www.physionet.org/physiobank/database/challenge/2018/training/tr03-0005/.hea'"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import wfdb\n",
    "# Demo 1 - Read a wfdb record using the 'rdrecord' function into a wfdb.Record object.\n",
    "# Plot the signals, and show the data.\n",
    "record = wfdb.rdrecord('https://www.physionet.org/physiobank/database/challenge/2018/training/tr03-0005/') \n",
    "wfdb.plot_wfdb(record=record, title='Record a103l from Physionet Challenge 2015') \n",
    "display(record.__dict__)\n",
    "\n",
    "\n",
    "# Can also read the same files hosted on Physiobank https://physionet.org/physiobank/database/\n",
    "# in the challenge/2015/training/ database subdirectory. Full url = https://physionet.org/physiobank/database/challenge/2015/training/\n",
    "record2 = wfdb.rdrecord('a103l', pb_dir = 'https://physionet.org/physiobank/database/challenge/2015/training/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfdb.rdann('sample-data/100', 'atr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isabelmetzger/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>CoverageLevel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>@_derricklamar @sly9297 has a root canal sched...</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>In such horrible dental pain. Have good insura...</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>omg I am in sooo much fucking pain. I have a d...</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>\"@leahhfragg @RandeeeGE LMFAOOO \"I can't affor...</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>I do not want to go to the dentist tomorrow. H...</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  id                                               text  \\\n",
       "0           1   1  @_derricklamar @sly9297 has a root canal sched...   \n",
       "1           2   2  In such horrible dental pain. Have good insura...   \n",
       "2           3   3  omg I am in sooo much fucking pain. I have a d...   \n",
       "3           4   4  \"@leahhfragg @RandeeeGE LMFAOOO \"I can't affor...   \n",
       "4           5   5  I do not want to go to the dentist tomorrow. H...   \n",
       "\n",
       "  CoverageLevel  \n",
       "0          High  \n",
       "1          High  \n",
       "2          High  \n",
       "3          High  \n",
       "4          High  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@_derricklamar @sly9297 has a root canal sched...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In such horrible dental pain. Have good insura...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>omg I am in sooo much fucking pain. I have a d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"@leahhfragg @RandeeeGE LMFAOOO \"I can't affor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I do not want to go to the dentist tomorrow. H...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  @_derricklamar @sly9297 has a root canal sched...      1\n",
       "1  In such horrible dental pain. Have good insura...      1\n",
       "2  omg I am in sooo much fucking pain. I have a d...      1\n",
       "3  \"@leahhfragg @RandeeeGE LMFAOOO \"I can't affor...      1\n",
       "4  I do not want to go to the dentist tomorrow. H...      1"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1502 entries, 0 to 1501\n",
      "Data columns (total 2 columns):\n",
      "text     1502 non-null object\n",
      "label    1502 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 23.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [1]\n",
      " [1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isabelmetzger/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "X = df.text\n",
    "y = df.label\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(y)\n",
    "Y = y.reshape(-1,1)\n",
    "print(Y[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "520     gonna see about maybe getting something less s...\n",
       "1228    @Horsewhispers : Oh believe me. I pop NSAIDs l...\n",
       "1246    @DemonikaDevour sorry, i need a dentist but a ...\n",
       "1464    And if it is wisdom teeth, then I'm fucked. No...\n",
       "1468    You know what sucks? Having like 3 cavities bu...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(X,y,test_size=0.2)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 5000\n",
    "max_len = 500\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X_train)\n",
    "sequences = tok.texts_to_sequences(X_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embedding_18 (Embedding)     (None, 500, 100)          500000    \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 559,137\n",
      "Trainable params: 559,137\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def binary_accuracy(y_true, y_pred):\n",
    "    return K.mean(K.equal(y_true, K.round(y_pred)))\n",
    "\n",
    "\n",
    "def categorical_accuracy(y_true, y_pred):\n",
    "    return K.mean(K.equal(K.argmax(y_true, axis=-1),\n",
    "                          K.argmax(y_pred, axis=-1)))\n",
    "\n",
    "\n",
    "def sparse_categorical_accuracy(y_true, y_pred):\n",
    "    return K.mean(K.equal(K.max(y_true, axis=-1),\n",
    "                          K.cast(K.argmax(y_pred, axis=-1), K.floatx())))\n",
    "\n",
    "\n",
    "def top_k_categorical_accuracy(y_true, y_pred, k=5):\n",
    "    return K.mean(K.in_top_k(y_pred, K.argmax(y_true, axis=-1), k))\n",
    "\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true))\n",
    "\n",
    "\n",
    "def mean_absolute_error(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true))\n",
    "\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    diff = K.abs((y_true - y_pred) / K.clip(K.abs(y_true),\n",
    "                                            K.epsilon(),\n",
    "                                            None))\n",
    "    return 100. * K.mean(diff)\n",
    "\n",
    "\n",
    "def mean_squared_logarithmic_error(y_true, y_pred):\n",
    "    first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n",
    "    second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n",
    "    return K.mean(K.square(first_log - second_log))\n",
    "\n",
    "\n",
    "def hinge(y_true, y_pred):\n",
    "    return K.mean(K.maximum(1. - y_true * y_pred, 0.))\n",
    "\n",
    "\n",
    "def squared_hinge(y_true, y_pred):\n",
    "    return K.mean(K.square(K.maximum(1. - y_true * y_pred, 0.)))\n",
    "\n",
    "\n",
    "def categorical_crossentropy(y_true, y_pred):\n",
    "    return K.mean(K.categorical_crossentropy(y_pred, y_true))\n",
    "\n",
    "\n",
    "def sparse_categorical_crossentropy(y_true, y_pred):\n",
    "    return K.mean(K.sparse_categorical_crossentropy(y_pred, y_true))\n",
    "\n",
    "\n",
    "def binary_crossentropy(y_true, y_pred):\n",
    "    return K.mean(K.binary_crossentropy(y_pred, y_true))\n",
    "\n",
    "\n",
    "def kullback_leibler_divergence(y_true, y_pred):\n",
    "    y_true = K.clip(y_true, K.epsilon(), 1)\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1)\n",
    "    return K.mean(K.sum(y_true * K.log(y_true / y_pred), axis=-1))\n",
    "\n",
    "\n",
    "def poisson(y_true, y_pred):\n",
    "    return K.mean(y_pred - y_true * K.log(y_pred + K.epsilon()))\n",
    "\n",
    "\n",
    "def cosine_proximity(y_true, y_pred):\n",
    "    y_true = K.l2_normalize(y_true, axis=-1)\n",
    "    y_pred = K.l2_normalize(y_pred, axis=-1)\n",
    "    return -K.mean(y_true * y_pred)\n",
    "\n",
    "\n",
    "def matthews_correlation(y_true, y_pred):\n",
    "    \"\"\"Matthews correlation metric.\n",
    "    It is only computed as a batch-wise average, not globally.\n",
    "    Computes the Matthews correlation coefficient measure for quality\n",
    "    of binary classification problems.\n",
    "    \"\"\"\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos\n",
    "\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    tn = K.sum(y_neg * y_pred_neg)\n",
    "\n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    "\n",
    "    numerator = (tp * tn - fp * fn)\n",
    "    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    return numerator / (denominator + K.epsilon())\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "    Only computes a batch-wise average of precision.\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "    Only computes a batch-wise average of recall.\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def fbeta_score(y_true, y_pred, beta=1):\n",
    "    \"\"\"Computes the F score.\n",
    "    The F score is the weighted harmonic mean of precision and recall.\n",
    "    Here it is only computed as a batch-wise average, not globally.\n",
    "    This is useful for multi-label classification, where input samples can be\n",
    "    classified as sets of labels. By only using accuracy (precision) a model\n",
    "    would achieve a perfect score by simply assigning every class to every\n",
    "    input. In order to avoid this, a metric should penalize incorrect class\n",
    "    assignments as well (recall). The F-beta score (ranged from 0.0 to 1.0)\n",
    "    computes this, as a weighted mean of the proportion of correct class\n",
    "    assignments vs. the proportion of incorrect class assignments.\n",
    "    With beta = 1, this is equivalent to a F-measure. With beta < 1, assigning\n",
    "    correct classes becomes more important, and with beta > 1 the metric is\n",
    "    instead weighted towards penalizing incorrect class assignments.\n",
    "    \"\"\"\n",
    "    if beta < 0:\n",
    "        raise ValueError('The lowest choosable beta is zero (only precision).')\n",
    "\n",
    "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
    "    return fbeta_score\n",
    "\n",
    "\n",
    "def fmeasure(y_true, y_pred):\n",
    "    \"\"\"Computes the f-measure, the harmonic mean of precision and recall.\n",
    "    Here it is only computed as a batch-wise average, not globally.\n",
    "    \"\"\"\n",
    "    return fbeta_score(y_true, y_pred, beta=1)\n",
    "\n",
    "\n",
    "# aliases\n",
    "mse = MSE = mean_squared_error\n",
    "mae = MAE = mean_absolute_error\n",
    "mape = MAPE = mean_absolute_percentage_error\n",
    "msle = MSLE = mean_squared_logarithmic_error\n",
    "cosine = cosine_proximity\n",
    "fscore = f1score = fmeasure\n",
    "\n",
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,100,input_length=max_len)(inputs)\n",
    "    layer = LSTM(64)(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model\n",
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy', recall, precision, fmeasure])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "520     0\n",
       "1228    1\n",
       "1246    1\n",
       "1464    1\n",
       "1468    1\n",
       "711     1\n",
       "763     0\n",
       "824     1\n",
       "822     1\n",
       "166     1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 960 samples, validate on 241 samples\n",
      "Epoch 1/3\n",
      "960/960 [==============================] - 6s 7ms/step - loss: 0.6635 - acc: 0.6490 - recall: 0.9302 - precision: 0.6692 - fmeasure: 0.7722 - val_loss: 0.6450 - val_acc: 0.6598 - val_recall: 1.0000 - val_precision: 0.6598 - val_fmeasure: 0.7948\n",
      "Epoch 2/3\n",
      "960/960 [==============================] - 4s 5ms/step - loss: 0.6210 - acc: 0.6771 - recall: 1.0000 - precision: 0.6771 - fmeasure: 0.8073 - val_loss: 0.6430 - val_acc: 0.6598 - val_recall: 1.0000 - val_precision: 0.6598 - val_fmeasure: 0.7948\n",
      "Epoch 3/3\n",
      "960/960 [==============================] - 4s 4ms/step - loss: 0.6038 - acc: 0.6771 - recall: 1.0000 - precision: 0.6771 - fmeasure: 0.8072 - val_loss: 0.6471 - val_acc: 0.6598 - val_recall: 1.0000 - val_precision: 0.6598 - val_fmeasure: 0.7948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a3387d080>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(sequences_matrix,Y_train,batch_size=128,epochs=3,\n",
    "          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301/301 [==============================] - 1s 2ms/step\n",
      "Test set\n",
      "  Loss: 0.613\n",
      "  Accuracy: 0.701\n",
      "  Recall: 1.000\n",
      "  Precision: 0.701\n",
      "  fmeasure: 0.824\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['loss', 'acc', 'recall', 'precision', 'fmeasure']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sequences = tok.texts_to_sequences(X_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "accr = model.evaluate(test_sequences_matrix,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n  Recall: {:0.3f}\\n  Precision: {:0.3f}\\n  fmeasure: {:0.3f}'.format(accr[0],accr[1],\n",
    "                                                                                                                            accr[2], accr[3], accr[4]))\n",
    "accr  #Accuracy: 0.702\n",
    "from sklearn.metrics import f1_score\n",
    "predictions = model.predict(test_sequences_matrix,  verbose=0)  #  Accuracy: 0.702\n",
    "model.metrics_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test set\n",
    "  Loss: 0.633\n",
    "  Accuracy: 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 216,405\n",
      "Trainable params: 216,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "1201/1201 [==============================] - 6s 5ms/step - loss: 0.6563 - acc: 0.6719 - recall: 0.9878 - precision: 0.6784 - fmeasure: 0.8003\n",
      "Epoch 2/3\n",
      "1201/1201 [==============================] - 5s 4ms/step - loss: 0.6270 - acc: 0.6736 - recall: 1.0000 - precision: 0.6736 - fmeasure: 0.8031\n",
      "Epoch 3/3\n",
      "1201/1201 [==============================] - 4s 4ms/step - loss: 0.6161 - acc: 0.6736 - recall: 1.0000 - precision: 0.6736 - fmeasure: 0.8029\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a35fc4fd0>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "tok = Tokenizer(num_words=top_words)\n",
    "tok.fit_on_texts(X_train)\n",
    "sequences = tok.texts_to_sequences(X_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_review_length)\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', recall, precision, fmeasure])\n",
    "print(model.summary())\n",
    "model.fit(sequences_matrix,Y_train, epochs=3, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "#scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "#print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301/301 [==============================] - 1s 4ms/step\n",
      "Test set\n",
      "  Loss: 0.610\n",
      "  Accuracy: 0.701\n",
      "  Recall: 1.000\n",
      "  Precision: 0.701\n",
      "  fmeasure: 0.824\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['loss', 'acc', 'recall', 'precision', 'fmeasure']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sequences = tok.texts_to_sequences(X_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_review_length)\n",
    "accr = model.evaluate(test_sequences_matrix,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n  Recall: {:0.3f}\\n  Precision: {:0.3f}\\n  fmeasure: {:0.3f}'.format(accr[0],accr[1],\n",
    "                                                                                                                            accr[2], accr[3], accr[4]))\n",
    "accr  #Accuracy: 0.702\n",
    "from sklearn.metrics import f1_score\n",
    "predictions = model.predict(test_sequences_matrix,  verbose=0)  #  Accuracy: 0.702\n",
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_20 (Embedding)     (None, 100, 32)           32000     \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 85,301\n",
      "Trainable params: 85,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "1201/1201 [==============================] - 4s 3ms/step - loss: 0.6519 - acc: 0.6678 - recall: 0.9784 - precision: 0.6712 - fmeasure: 0.7947\n",
      "Epoch 2/3\n",
      "1201/1201 [==============================] - 2s 2ms/step - loss: 0.6333 - acc: 0.6736 - recall: 1.0000 - precision: 0.6736 - fmeasure: 0.8033\n",
      "Epoch 3/3\n",
      "1201/1201 [==============================] - 2s 2ms/step - loss: 0.6235 - acc: 0.6736 - recall: 1.0000 - precision: 0.6736 - fmeasure: 0.8041\n",
      "Test set\n",
      "  Loss: 0.610\n",
      "  Accuracy: 0.701\n",
      "  Recall: 1.000\n",
      "  Precision: 0.701\n",
      "  fmeasure: 0.824\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(12)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 1000\n",
    "\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 100\n",
    "tok = Tokenizer(num_words=top_words)\n",
    "tok.fit_on_texts(X_train)\n",
    "sequences = tok.texts_to_sequences(X_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_review_length)\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', recall, precision, fmeasure])\n",
    "print(model.summary())\n",
    "model.fit(sequences_matrix, Y_train, epochs=3, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "test_sequences = tok.texts_to_sequences(X_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_review_length)\n",
    "accr = model.evaluate(test_sequences_matrix,Y_test, verbose=0)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n  Recall: {:0.3f}\\n  Precision: {:0.3f}\\n  fmeasure: {:0.3f}'.format(accr[0],accr[1],\n",
    "                                                                                                                            accr[2], accr[3], accr[4]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Build model...\n",
      "Train...\n",
      "Train on 1201 samples, validate on 301 samples\n",
      "Epoch 1/2\n",
      "1201/1201 [==============================] - 5s 5ms/step - loss: 0.6446 - acc: 0.6736 - recall: 1.0000 - precision: 0.6736 - fmeasure: 0.8018 - val_loss: 0.6140 - val_acc: 0.7010 - val_recall: 1.0000 - val_precision: 0.7010 - val_fmeasure: 0.8235\n",
      "Epoch 2/2\n",
      "1201/1201 [==============================] - 3s 3ms/step - loss: 0.6224 - acc: 0.6736 - recall: 0.9992 - precision: 0.6736 - fmeasure: 0.8019 - val_loss: 0.6107 - val_acc: 0.7010 - val_recall: 1.0000 - val_precision: 0.7010 - val_fmeasure: 0.8235\n",
      "301/301 [==============================] - 0s 243us/step\n",
      "Test score: 0.6106568771937361\n",
      "Test accuracy: 0.700996683681526\n",
      "Test recall: 0.9999999996039558\n",
      "Test precision: 0.7009966832854819\n",
      "Test fmeasure: 0.8234838597798269\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "\n",
    "# Embedding\n",
    "max_features = 20000\n",
    "max_len = 100\n",
    "embedding_size = 128\n",
    "tok = Tokenizer(num_words=max_features)\n",
    "tok.fit_on_texts(X_train)\n",
    "sequences = tok.texts_to_sequences(X_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "# Convolution\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 70\n",
    "\n",
    "# Training\n",
    "batch_size = 30\n",
    "epochs = 2\n",
    "\n",
    "'''\n",
    "Note:\n",
    "batch_size is highly sensitive.\n",
    "Only 2 epochs are needed as the dataset is very small.\n",
    "'''\n",
    "\n",
    "print('Loading data...')\n",
    "\n",
    "\n",
    "\n",
    "test_sequences = tok.texts_to_sequences(X_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "print('Build model...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_size, input_length=max_len))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "model.add(MaxPooling1D(pool_size=pool_size))\n",
    "model.add(LSTM(lstm_output_size))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy',recall, precision, fmeasure])\n",
    "#, recall, precision, fmeasure]\n",
    "print('Train...')\n",
    "model.fit(sequences_matrix, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(test_sequences_matrix, Y_test))\n",
    "score, acc,rec, precision, fscore  = model.evaluate(test_sequences_matrix, Y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "print('Test recall:', rec)\n",
    "print('Test precision:', precision)\n",
    "print('Test fmeasure:', fscore)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers.noise import AlphaDropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "max_words = 1000\n",
    "batch_size = 16\n",
    "epochs = 40\n",
    "plot = True\n",
    "\n",
    "\n",
    "def create_network(n_dense=6,\n",
    "                   dense_units=16,\n",
    "                   activation='selu',\n",
    "                   dropout=AlphaDropout,\n",
    "                   dropout_rate=0.1,\n",
    "                   kernel_initializer='lecun_normal',\n",
    "                   optimizer='adam',\n",
    "                   num_classes=2,\n",
    "                   max_words=max_words):\n",
    "    \"\"\"Generic function to create a fully-connected neural network.\n",
    "    # Arguments\n",
    "        n_dense: int > 0. Number of dense layers.\n",
    "        dense_units: int > 0. Number of dense units per layer.\n",
    "        dropout: keras.layers.Layer. A dropout layer to apply.\n",
    "        dropout_rate: 0 <= float <= 1. The rate of dropout.\n",
    "        kernel_initializer: str. The initializer for the weights.\n",
    "        optimizer: str/keras.optimizers.Optimizer. The optimizer to use.\n",
    "        num_classes: int > 0. The number of classes to predict.\n",
    "        max_words: int > 0. The maximum number of words per data point.\n",
    "    # Returns\n",
    "        A Keras model instance (compiled).\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(dense_units, input_shape=(max_words,),\n",
    "                    kernel_initializer=kernel_initializer))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(dropout(dropout_rate))\n",
    "\n",
    "    for i in range(n_dense - 1):\n",
    "        model.add(Dense(dense_units, kernel_initializer=kernel_initializer))\n",
    "        model.add(Activation(activation))\n",
    "        model.add(dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "network1 = {\n",
    "    'n_dense': 6,\n",
    "    'dense_units': 16,\n",
    "    'activation': 'relu',\n",
    "    'dropout': Dropout,\n",
    "    'dropout_rate': 0.5,\n",
    "    'kernel_initializer': 'glorot_uniform',\n",
    "    'optimizer': 'sgd'\n",
    "}\n",
    "\n",
    "network2 = {\n",
    "    'n_dense': 6,\n",
    "    'dense_units': 16,\n",
    "    'activation': 'selu',\n",
    "    'dropout': AlphaDropout,\n",
    "    'dropout_rate': 0.1,\n",
    "    'kernel_initializer': 'lecun_normal',\n",
    "    'optimizer': 'sgd'\n",
    "}\n",
    "\n",
    "print('Loading data...')\n",
    "\n",
    "print(len(sequences_matrix), 'train sequences')\n",
    "print(len(test_sequences_matrix), 'test sequences')\n",
    "\n",
    "num_classes = np.max(Y_train) + 1\n",
    "print(num_classes, 'classes')\n",
    "\n",
    "print('Vectorizing sequence data...')\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "x_train = tokenizer.sequences_to_matrix(X_train, mode='binary')\n",
    "x_test = tokenizer.sequences_to_matrix(X_test, mode='binary')\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Convert class vector to binary class matrix '\n",
    "      '(for use with categorical_crossentropy)')\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "\n",
    "print('\\nBuilding network 1...')\n",
    "\n",
    "model1 = create_network(num_classes=num_classes, **network1)\n",
    "history_model1 = model1.fit(x_train,\n",
    "                            y_train,\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=epochs,\n",
    "                            verbose=1,\n",
    "                            validation_split=0.1)\n",
    "\n",
    "score_model1 = model1.evaluate(x_test,\n",
    "                               y_test,\n",
    "                               batch_size=batch_size,\n",
    "                               verbose=1)\n",
    "\n",
    "\n",
    "print('\\nBuilding network 2...')\n",
    "model2 = create_network(num_classes=num_classes, **network2)\n",
    "\n",
    "history_model2 = model2.fit(x_train,\n",
    "                            y_train,\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=epochs,\n",
    "                            verbose=1,\n",
    "                            validation_split=0.1)\n",
    "\n",
    "score_model2 = model2.evaluate(x_test,\n",
    "                               y_test,\n",
    "                               batch_size=batch_size,\n",
    "                               verbose=1)\n",
    "\n",
    "print('\\nNetwork 1 results')\n",
    "print('Hyperparameters:', network1)\n",
    "print('Test score:', score_model1[0])\n",
    "print('Test accuracy:', score_model1[1])\n",
    "print('Network 2 results')\n",
    "print('Hyperparameters:', network2)\n",
    "print('Test score:', score_model2[0])\n",
    "print('Test accuracy:', score_model2[1])\n",
    "\n",
    "plt.plot(range(epochs),\n",
    "         history_model1.history['val_loss'],\n",
    "         'g-',\n",
    "         label='Network 1 Val Loss')\n",
    "plt.plot(range(epochs),\n",
    "         history_model2.history['val_loss'],\n",
    "         'r-',\n",
    "         label='Network 2 Val Loss')\n",
    "plt.plot(range(epochs),\n",
    "         history_model1.history['loss'],\n",
    "         'g--',\n",
    "         label='Network 1 Loss')\n",
    "plt.plot(range(epochs),\n",
    "         history_model2.history['loss'],\n",
    "         'r--',\n",
    "         label='Network 2 Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('comparison_of_networks.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
